{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#改查詢\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "import xlwt\n",
    "import xlrd\n",
    "\n",
    "  \n",
    "workbook = xlwt.Workbook(encoding='utf-8')  \n",
    "booksheet = workbook.add_sheet('Sheet 1', cell_overwrite_ok=True)  \n",
    "#write dict\n",
    "\n",
    "\n",
    "\n",
    "res= requests.get(\"http://sports.williamhill.com/bet/zh-hk/betting/g/6339944/%E7%8D%A8%E8%B4%8F%E7%9B%A4.html\")\n",
    "soup=BeautifulSoup(res.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def l1():   \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[6].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[7].text[19:23])\n",
    "    return k\n",
    "\n",
    "def l2():   \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[8].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[9].text[19:23])\n",
    "    return k \n",
    "def l3():   \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[10].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[11].text[19:23])\n",
    "    return k \n",
    "\n",
    "def l4():  \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[4].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[5].text[19:23])\n",
    "    return k \n",
    "    \n",
    "\n",
    "def l5(): \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[2].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[3].text[19:23])\n",
    "    return k \n",
    "\n",
    "def l6():\n",
    "    k=[]\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[0].text[19:23]))\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[1].text[19:23])\n",
    "    return k \n",
    "\n",
    "\n",
    "\n",
    "def dictinal1():\n",
    "    a=dict()\n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[0]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[0].text.strip('\\n').replace('\\n', ''))]=l6() \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[1]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[1].text.strip('\\n').replace('\\n', ''))]=l5()     \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[2]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[2].text.strip('\\n').replace('\\n', ''))]=l4()   \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[3]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[3].text.strip('\\n').replace('\\n', ''))]=l1()   \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[4]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[4].text.strip('\\n').replace('\\n', ''))]=l2()   \n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(list(dictinal1().keys()))):\n",
    "    booksheet.write(i,0,list(dictinal1().keys())[i])\n",
    "    booksheet.write(i,1,dictinal1()[list(dictinal1().keys())[i]][0])\n",
    "    booksheet.write(i,2,dictinal1()[list(dictinal1().keys())[i]][1])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "res= requests.get(\"http://sports.williamhill.com/bet/zh-hk/betting/g/6339945/%E8%AE%93%E5%88%86%E7%9B%A4.html\")\n",
    "soup=BeautifulSoup(res.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def l1():   \n",
    "    k=[]\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[6].text[19:23]))\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[7].text[19:23]))\n",
    "    return k\n",
    "\n",
    "def l2():   \n",
    "    k=[]\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[8].text[19:23]))\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[9].text[19:23]))\n",
    "    return k \n",
    "def l3():   \n",
    "    k=[]\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[10].text[19:23]))\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[11].text[19:23]))\n",
    "    return k \n",
    "\n",
    "def l4():  \n",
    "    k=[]\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[4].text[19:23]))\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[5].text[19:23]))\n",
    "    return k \n",
    "    \n",
    "\n",
    "def l5(): \n",
    "    k=[]\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[2].text[19:23]))\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[3].text[19:23]))\n",
    "    return k \n",
    "\n",
    "def l6():\n",
    "    k=[]\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[0].text[19:23]))\n",
    "    k.append(float(soup.findAll(\"div\",{\"class\":\"eventprice\"})[1].text[19:23]))\n",
    "    return k \n",
    "\n",
    "\n",
    "\n",
    "def dictinal2():\n",
    "    a=dict()\n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[0]!=None:\n",
    "        a[soup.findAll(\"td\",{\"class\":\"CentrePad\"})[0].text.strip('\\n').replace('\\n', '').replace('\\t', '')]=l6() \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[1]!=None:\n",
    "        a[soup.findAll(\"td\",{\"class\":\"CentrePad\"})[1].text.strip('\\n').replace('\\n', '').replace('\\t', '')]=l5()     \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[2]!=None:\n",
    "        a[soup.findAll(\"td\",{\"class\":\"CentrePad\"})[2].text.strip('\\n').replace('\\n', '').replace('\\t', '')]=l4()   \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[3]!=None:\n",
    "        a[soup.findAll(\"td\",{\"class\":\"CentrePad\"})[3].text.strip('\\n').replace('\\n', '').replace('\\t', '')]=l1()   \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[4]!=None:\n",
    "        a[soup.findAll(\"td\",{\"class\":\"CentrePad\"})[4].text.strip('\\n').replace('\\n', '').replace('\\t', '')]=l2()   \n",
    "    ## if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[5]!=None:\n",
    "        ##a[soup.findAll(\"td\",{\"class\":\"CentrePad\"})[5].text.strip('\\n').replace('\\n', '').replace('\\t', '')]=l3()    \n",
    "    return a\n",
    "\n",
    "for i in range(0, len(list(dictinal2().keys()))):\n",
    "    booksheet.write(i+8,0,list(dictinal2().keys())[i])\n",
    "    booksheet.write(i+8,1,dictinal2()[list(dictinal2().keys())[i]][0])\n",
    "    booksheet.write(i+8,2,dictinal2()[list(dictinal2().keys())[i]][1])\n",
    "\n",
    "\n",
    "\n",
    "res= requests.get(\"http://sports.williamhill.com/bet/zh-hk/betting/g/6339946/%e7%b8%bd%e5%be%97%e5%88%86.html\")\n",
    "soup=BeautifulSoup(res.text)\n",
    "\n",
    "\n",
    "def f(n):\n",
    "    a=''\n",
    "    if soup.find(\"div\", id=n) !=None:\n",
    "        for i in range(19, 23):\n",
    "            a+=soup.find(\"div\", id=n).text[i]\n",
    "    return a\n",
    "\n",
    "\n",
    "def l1():   \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[6].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[7].text[19:23])\n",
    "    return k\n",
    "\n",
    "def l2():   \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[8].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[9].text[19:23])\n",
    "    return k \n",
    "def l3():   \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[10].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[11].text[19:23])\n",
    "    return k \n",
    "\n",
    "def l4():  \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[4].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[5].text[19:23])\n",
    "    return k \n",
    "    \n",
    "\n",
    "def l5(): \n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[2].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[3].text[19:23])\n",
    "    return k \n",
    "\n",
    "def l6():\n",
    "    k=[]\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[0].text[19:23])\n",
    "    k.append(soup.findAll(\"div\",{\"class\":\"eventprice\"})[1].text[19:23])\n",
    "    return k \n",
    "\n",
    "\n",
    "\n",
    "def dictinal3():\n",
    "    a=dict()\n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[0]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[0].text.strip('\\n').replace('\\n', ''))]=l6() \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[2]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[2].text.strip('\\n').replace('\\n', ''))]=l5()     \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[4]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[4].text.strip('\\n').replace('\\n', ''))]=l4()   \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[6]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[6].text.strip('\\n').replace('\\n', ''))]=l1()   \n",
    "    if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[8]!=None:\n",
    "        a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[8].text.strip('\\n').replace('\\n', ''))]=l2()   \n",
    "     ##if soup.findAll(\"td\",{\"class\":\"CentrePad\"})[10]!=None:\n",
    "         ##a[unicodedata.normalize(\"NFKD\", soup.findAll(\"td\",{\"class\":\"CentrePad\"})[10].text.strip('\\n').replace('\\n', ''))]=l3()    \n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(list(dictinal3().keys()))):\n",
    "    booksheet.write(i+16,0,list(dictinal3().keys())[i])\n",
    "    booksheet.write(i+16,1,dictinal3()[list(dictinal3().keys())[i]][0])\n",
    "    booksheet.write(i+16,2,dictinal3()[list(dictinal3().keys())[i]][1])\n",
    "    booksheet.write(i+16,3,soup.findAll(\"td\",{\"class\":\"CentrePad\"})[2*i+1].text)   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "dictinal3()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "  \n",
    "workbook.save('npb1.xls') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.findAll(\"td\",{\"class\":\"CentrePad\"})[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2d6890eb4bf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"td\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"CentrePad\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "soup.findAll(\"td\",{\"class\":\"CentrePad\"})[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
